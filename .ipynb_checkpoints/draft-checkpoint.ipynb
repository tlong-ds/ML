{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.32222551950101896\n",
      "Epoch 100, Loss: 0.24985677531669398\n",
      "Epoch 200, Loss: 0.24965289468475027\n",
      "Epoch 300, Loss: 0.2494604778997051\n",
      "Epoch 400, Loss: 0.2492707731939513\n",
      "Epoch 500, Loss: 0.24907797188348108\n",
      "Epoch 600, Loss: 0.2488767156669774\n",
      "Epoch 700, Loss: 0.24866181592446246\n",
      "Epoch 800, Loss: 0.24842802522575191\n",
      "Epoch 900, Loss: 0.24816984413011492\n",
      "Epoch 1000, Loss: 0.2478813523255476\n",
      "Epoch 1100, Loss: 0.24755605771766\n",
      "Epoch 1200, Loss: 0.24718676081800628\n",
      "Epoch 1300, Loss: 0.24676543504933857\n",
      "Epoch 1400, Loss: 0.24628312660404647\n",
      "Epoch 1500, Loss: 0.24572988040974925\n",
      "Epoch 1600, Loss: 0.24509470171053843\n",
      "Epoch 1700, Loss: 0.2443655659806519\n",
      "Epoch 1800, Loss: 0.2435294937434727\n",
      "Epoch 1900, Loss: 0.2425727119896959\n",
      "Epoch 2000, Loss: 0.24148093097106507\n",
      "Epoch 2100, Loss: 0.24023977442224098\n",
      "Epoch 2200, Loss: 0.23883541134065506\n",
      "Epoch 2300, Loss: 0.2372554435425175\n",
      "Epoch 2400, Loss: 0.23549009534377188\n",
      "Epoch 2500, Loss: 0.23353371462921102\n",
      "Epoch 2600, Loss: 0.23138651216333536\n",
      "Epoch 2700, Loss: 0.22905633319216545\n",
      "Epoch 2800, Loss: 0.2265600965449649\n",
      "Epoch 2900, Loss: 0.2239244194539373\n",
      "Epoch 3000, Loss: 0.2211849702399789\n",
      "Epoch 3100, Loss: 0.2183843297153113\n",
      "Epoch 3200, Loss: 0.2155685643737975\n",
      "Epoch 3300, Loss: 0.21278315227407105\n",
      "Epoch 3400, Loss: 0.21006913233449598\n",
      "Epoch 3500, Loss: 0.2074602449443943\n",
      "Epoch 3600, Loss: 0.20498146357848257\n",
      "Epoch 3700, Loss: 0.2026488845463709\n",
      "Epoch 3800, Loss: 0.20047063349661962\n",
      "Epoch 3900, Loss: 0.19844833694244496\n",
      "Epoch 4000, Loss: 0.19657875766468108\n",
      "Epoch 4100, Loss: 0.19485532041824877\n",
      "Epoch 4200, Loss: 0.19326938683470302\n",
      "Epoch 4300, Loss: 0.19181123835632155\n",
      "Epoch 4400, Loss: 0.190470785695599\n",
      "Epoch 4500, Loss: 0.18923805043169317\n",
      "Epoch 4600, Loss: 0.18810347070113648\n",
      "Epoch 4700, Loss: 0.18705807849802236\n",
      "Epoch 4800, Loss: 0.18609358747167618\n",
      "Epoch 4900, Loss: 0.18520242093950956\n",
      "Epoch 5000, Loss: 0.18437770176888613\n",
      "Epoch 5100, Loss: 0.18361321934130037\n",
      "Epoch 5200, Loss: 0.18290338396074468\n",
      "Epoch 5300, Loss: 0.18224317555577932\n",
      "Epoch 5400, Loss: 0.1816280910552994\n",
      "Epoch 5500, Loss: 0.1810540931223128\n",
      "Epoch 5600, Loss: 0.18051756179024933\n",
      "Epoch 5700, Loss: 0.1800152497965642\n",
      "Epoch 5800, Loss: 0.17954424192728308\n",
      "Epoch 5900, Loss: 0.17910191838716824\n",
      "Epoch 6000, Loss: 0.17868592203283046\n",
      "Epoch 6100, Loss: 0.1782941292086711\n",
      "Epoch 6200, Loss: 0.1779246238795929\n",
      "Epoch 6300, Loss: 0.17757567474084707\n",
      "Epoch 6400, Loss: 0.17724571499142008\n",
      "Epoch 6500, Loss: 0.17693332447465177\n",
      "Epoch 6600, Loss: 0.17663721391286769\n",
      "Epoch 6700, Loss: 0.1763562109882701\n",
      "Epoch 6800, Loss: 0.17608924804803422\n",
      "Epoch 6900, Loss: 0.1758353512362556\n",
      "Epoch 7000, Loss: 0.17559363087838692\n",
      "Epoch 7100, Loss: 0.17536327296478868\n",
      "Epoch 7200, Loss: 0.17514353159888094\n",
      "Epoch 7300, Loss: 0.17493372229217108\n",
      "Epoch 7400, Loss: 0.17473321600326636\n",
      "Epoch 7500, Loss: 0.1745414338310091\n",
      "Epoch 7600, Loss: 0.17435784228328066\n",
      "Epoch 7700, Loss: 0.17418194905297374\n",
      "Epoch 7800, Loss: 0.17401329924130443\n",
      "Epoch 7900, Loss: 0.17385147197618334\n",
      "Epoch 8000, Loss: 0.17369607737992376\n",
      "Epoch 8100, Loss: 0.1735467538462668\n",
      "Epoch 8200, Loss: 0.17340316559166088\n",
      "Epoch 8300, Loss: 0.17326500045004295\n",
      "Epoch 8400, Loss: 0.1731319678841129\n",
      "Epoch 8500, Loss: 0.17300379718936143\n",
      "Epoch 8600, Loss: 0.17288023586995013\n",
      "Epoch 8700, Loss: 0.1727610481680223\n",
      "Epoch 8800, Loss: 0.17264601373018632\n",
      "Epoch 8900, Loss: 0.17253492639680457\n",
      "Epoch 9000, Loss: 0.1724275931013733\n",
      "Epoch 9100, Loss: 0.17232383286872727\n",
      "Epoch 9200, Loss: 0.17222347590207465\n",
      "Epoch 9300, Loss: 0.17212636274998094\n",
      "Epoch 9400, Loss: 0.17203234354540164\n",
      "Epoch 9500, Loss: 0.17194127730972575\n",
      "Epoch 9600, Loss: 0.17185303131555318\n",
      "Epoch 9700, Loss: 0.17176748050260102\n",
      "Epoch 9800, Loss: 0.17168450694172216\n",
      "Epoch 9900, Loss: 0.1716039993425528\n",
      "Input: [0 0], Predicted Output: [0.07749422]\n",
      "Input: [0 1], Predicted Output: [0.66012729]\n",
      "Input: [1 0], Predicted Output: [0.66013624]\n",
      "Input: [1 1], Predicted Output: [0.67013224]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize the neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.bias_hidden = np.random.rand(hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "        self.bias_output = np.random.rand(output_size)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # Forward pass\n",
    "        self.hidden_layer_input = np.dot(x, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
    "\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
    "        return self.output_layer_output\n",
    "\n",
    "    def backpropagate(self, x, y, learning_rate):\n",
    "        # Calculate error\n",
    "        output_error = y - self.output_layer_output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output_layer_output)\n",
    "\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0) * learning_rate\n",
    "        self.weights_input_hidden += x.T.dot(hidden_delta) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            self.feedforward(x)\n",
    "            self.backpropagate(x, y, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                loss = np.mean(np.square(y - self.output_layer_output))\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input data: 4 samples with 2 features each\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    # Output data: XOR function\n",
    "    y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "    # Create and train the neural network\n",
    "    nn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "    nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
    "\n",
    "    # Test the neural network\n",
    "    for sample in X:\n",
    "        print(f\"Input: {sample}, Predicted Output: {nn.feedforward(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a Sequential model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m      6\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,)),  \u001b[38;5;66;03m# Hidden layer with 64 units\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     Dense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)                   \u001b[38;5;66;03m# Output layer with 10 units (e.g., for classification)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(100,)),  # Hidden layer with 64 units\n",
    "    Dense(10, activation='softmax')                   # Output layer with 10 units (e.g., for classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
