{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "\n",
    "x = np.array([[200.0, 170.0]])\n",
    "layer_1 = Dense(units = 3, activation = 'sigmoid')\n",
    "a1 = layer_1(x)\n",
    "a1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[ 0.8185607  -0.8663975   1.0814984 ]\n",
      " [ 0.7494832  -0.79762244 -0.00185299]]\n",
      "Biases: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "weights, biases = layer_1.get_weights()\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Biases:\", biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (1000, 20)\n",
      "Labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# create data for multiclass classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_classes=3, random_state=42)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape) \n",
    "print(\"Labels shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=25, activation='relu', input_shape=(X.shape[1],)),\n",
    "    Dense(units=15, activation='relu'),\n",
    "    Dense(units=10, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), optimizer = Adam(0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/tensorflow/python/keras/engine/training.py:1137\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cluster_coordinator \u001b[38;5;241m=\u001b[39m cluster_coordinator\u001b[38;5;241m.\u001b[39mClusterCoordinator(\n\u001b[1;32m   1132\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy)\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope(), \\\n\u001b[1;32m   1135\u001b[0m      training_utils\u001b[38;5;241m.\u001b[39mRespectCompiledTrainableState(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1136\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1138\u001b[0m       x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m   1139\u001b[0m       y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   1140\u001b[0m       sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1141\u001b[0m       batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1142\u001b[0m       steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[1;32m   1143\u001b[0m       initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch,\n\u001b[1;32m   1144\u001b[0m       epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m   1145\u001b[0m       shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m   1146\u001b[0m       class_weight\u001b[38;5;241m=\u001b[39mclass_weight,\n\u001b[1;32m   1147\u001b[0m       max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[1;32m   1148\u001b[0m       workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[1;32m   1149\u001b[0m       use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   1150\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1151\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[1;32m   1153\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1397\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1396\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1151\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1149\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m-> 1151\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[1;32m   1153\u001b[0m     x,\n\u001b[1;32m   1154\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mdistribute_lib\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[1;32m   1164\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   1166\u001b[0m strategy \u001b[38;5;241m=\u001b[39m distribute_lib\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:987\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_data_adapter\u001b[39m(x, y):\n\u001b[1;32m    986\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m   adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    988\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    993\u001b[0m             _type_name(x), _type_name(y)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:706\u001b[0m, in \u001b[0;36mDatasetAdapter.can_handle\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcan_handle\u001b[39m(x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, (data_types\u001b[38;5;241m.\u001b[39mDatasetV1, data_types\u001b[38;5;241m.\u001b[39mDatasetV2)) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m           _is_distributed_dataset(x))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1696\u001b[0m, in \u001b[0;36m_is_distributed_dataset\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_distributed_dataset\u001b[39m(ds):\n\u001b[0;32m-> 1696\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ds, input_lib\u001b[38;5;241m.\u001b[39mDistributedDatasetInterface)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
