{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for Machine Learning Specialization, Course 1\n",
    "Visit via this link: \n",
    "<a href=\"https://www.coursera.org/learn/machine-learning/home/week/1\" target=\"_blank\"><button>Visit Course</button></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "- $\\text{x}$: data\n",
    "- $\\text{y}$: target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: (vectorize input variables and its coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "f_{\\text{w}, b}(\\text{x}) = \\text{w} \\cdot \\text{x} + b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $\\text{w} = [w_1, w_2, w_3, \\dots, w_m]$\n",
    "- $\\text{x}$: array with $m$ examples with $n$ features\n",
    "- $\\hat{\\text{y}} = f_{\\text{w}, b}$ is the predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Goal: adjusting $\\text{w}, b$ to minimize the cost function\n",
    "\\begin{equation}\n",
    "    J(\\text{w}, b) = \\frac{1}{2m}\\sum_{i = 0}^{m - 1}(f_{\\text{w}, b}(\\text{x}_i) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "- Pseudocode:\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\text{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\text{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, $n$ is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\text{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\text{w},b}(\\text{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\text{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\text{w},b}(\\text{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* $m$ is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\text{w},b}(\\text{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      x (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(x[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b):\n",
    "  \"\"\"\n",
    "  Computes the gradient for linear regression \n",
    "  Args:\n",
    "    x (ndarray (m,n)): Data, m examples with n features\n",
    "    y (ndarray (m,)) : target values\n",
    "    w (ndarray (n,)) : model parameters  \n",
    "    b (scalar)       : model parameter\n",
    "    \n",
    "  Returns:\n",
    "    dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "    dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "  \"\"\"\n",
    "  m, n = x.shape\n",
    "  dj_dw = np.zeros((n,))\n",
    "  dj_db = 0.\n",
    "\n",
    "  for i in range(m):\n",
    "    err = (np.dot(x[i], w) + b) - y[i]\n",
    "    dj_db += err\n",
    "    for j in range(n):\n",
    "      dj_dw[j] += err * x[i, j]\n",
    "  return dj_db / m, dj_dw / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in) \n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        dj_db, dj_dw = compute_gradient(x, y, w, b)   \n",
    "\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append(compute_cost(x, y, w, b))\n",
    "        \n",
    "        if np.all(np.abs(dj_dw) < epsilon) and abs(dj_db) < epsilon:\n",
    "            print(f\"Convergence reached at iteration {i}\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature engineering involves creating new features or modifying existing ones to improve the performance of machine learning models. This process can include:\n",
    "\n",
    "- **Creating interaction terms**: Combining two or more features to capture their interaction.\n",
    "- **Scaling and normalization**: Standardizing features to have a mean of 0 and a standard deviation of 1, or scaling them to a fixed range.\n",
    "\n",
    "In addition, (in later courses)\n",
    "- **Binning**: Converting continuous variables into categorical ones by dividing them into intervals.\n",
    "- **Encoding categorical variables**: Converting categorical variables into numerical format using techniques like one-hot encoding.\n",
    "- **Handling missing values**: Imputing missing values using mean, median, mode, or more sophisticated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling** is a technique used to normalize the range of independent variables or features of data. The goal of feature scaling is to ensure that all features contribute equally to the model's performance, especially when the features have different units or scales. \n",
    "\n",
    "There are 3 ways: Dividing the maximum value, mean normalization, z-score normalization (most common).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_norm(x):\n",
    "    mu = np.mean(x, axis = 0)\n",
    "    sigma = np.std(x, axis = 0)\n",
    "    x_norm = (x - mu)/sigma\n",
    "    return x_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling helps gradient descent run faster (less iterations), while get accurate result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial features** involve creating new features by raising existing features to a power. This can help capture non-linear relationships between features and the target variable. \n",
    "\n",
    "**Benefits**:\n",
    "- Captures non-linear relationships.\n",
    "- Can improve model performance if the underlying relationship is non-linear.\n",
    "\n",
    "**Drawbacks**:\n",
    "- Can lead to overfitting if the degree is too high.\n",
    "- Increases the complexity of the model.\n",
    "\n",
    "To reduce complexity, scale the polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues when training data:\n",
    "- Underfit: does not fit training set very well (high bias)\n",
    "- Overfit: fits training set extreme well (learning all noise, fluctuation of data), may perform worse for new examples. (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address:\n",
    "- Collect more training examples\n",
    "- Select features to include/exclude\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: adding penalty to model's loss function, discouraging overly complex models, improving generalization.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized cost function:\n",
    "\\begin{equation}\n",
    "    J(\\text{w}, b) = \\frac{1}{2m}\\sum_{i = 0}^{m - 1}(f_{\\text{w}, b}(\\text{x}_i) - y_i)^2 + \\frac{\\lambda}{2m}\\sum_{j = 0}^{n - 1}w_j^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(x, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      x (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    m, n = x.shape[0], len(w)\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(w[i], x[i]) + b\n",
    "        cost += (f_wb_i - y[i])**2\n",
    "    cost /= (2 * m)\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j] ** 2)\n",
    "    reg_cost *= (lambda_/(2 * m))\n",
    "    return cost + reg_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(x, y, w, b, lambda_):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros(n,)\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        err = (np.dot(w[i], x[i]) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * x[i][j]\n",
    "        dj_db += err\n",
    "\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] += (lambda_/m) * w[j]\n",
    "    \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the gradient function inside gredient descent algorithm, complete regularized gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "- $\\text{x}$: data\n",
    "- $\\text{y}$: target value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: substitute into sigmoid function\n",
    "\\begin{equation}\n",
    "    f_{\\text{w}, b}(\\text{x}) = g(\\text{w}\\text{x} + b) = \\frac{1}{1 + e^{-(\\text{w}\\text{x} + b)}}, \\quad f \\in (0, 1)\n",
    "\\end{equation}\n",
    "And conduct gradient descent, just like linear regression.\n",
    "Assume threshold is $0.5$\n",
    "- If predicted value is smaller, then predicted value $ = \\lceil 0.5 \\rceil = 0$.\n",
    "- if predicted value is greater, then predicted value $ = \\lfloor 0.5 \\rfloor = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret: probability that class/category is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + e ** (-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision boundary**: Graph of $z = 0$\n",
    "- If $z >= 0$, then predicted value is $1$.\n",
    "- If $z < 0$, then predicted value is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "\\begin{equation}\n",
    "L(f_{\\text{w}, b}(\\text{x}_i), y_i) = -y_i\\log(f_{\\text{w}, b}(\\text{x}_i)) - (1 - y_i)\\log(1 - f_{\\text{w}, b}(\\text{x}_i))\n",
    "\\end{equation}\n",
    "\n",
    "This is a convex function, meaning there's only 1 local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function:\n",
    "\\begin{equation}\n",
    "J(\\text{w}, b) = \\frac{1}{m}\\sum_{i = 1}^mL(f_{\\text{w}, b}(\\text{x}_i), y_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_cost(x, y, w, b):\n",
    "    m, n = x.shape\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(w[i], x[i]) + b)\n",
    "        cost += (-y[i] * np.log(f_wb_i) + (1 - y[i]) * np.log(1 - f_wb_i))\n",
    "    return cost / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent: same as Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_gradient(x, y, w, b):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros(n,)\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        err = sigmoid(np.dot(w[i], x[i]) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * x[i][j]\n",
    "        dj_db += err\n",
    "\n",
    "    return dj_db / m, dj_dw / m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized cost function:\n",
    "\\begin{equation}\n",
    "J(\\text{w}, b) = \\frac{1}{m}\\sum_{i = 1}^mL(f_{\\text{w}, b}(\\text{x}_i), y_i) + \\frac{\\lambda}{2m}\\sum_{j = 0}^{n - 1}w_j^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_cost_reg(x, y, w, b, lambda_):\n",
    "    m, n = x.shape\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(w[i], x[i]) + b)\n",
    "        cost += (-y[i] * np.log(f_wb_i) + (1 - y[i]) * np.log(1 - f_wb_i))\n",
    "\n",
    "    cost /= m\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j] ** 2)\n",
    "    reg_cost *= (lambda_/(2 * m))\n",
    "    return cost + reg_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regualarized gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_gradient_reg(x, y, w, b, lambda_):\n",
    "    m, n = x.shape\n",
    "    dj_dw = np.zeros(n,)\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        err = sigmoid(np.dot(w[i], x[i]) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += err * x[i][j]\n",
    "        dj_db += err\n",
    "    \n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] += (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
